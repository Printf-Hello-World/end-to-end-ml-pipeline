
# End to End Machine Learning Pipeline
This project implements a fully automated end-to-end machine learning pipeline that ingests raw data, performs preprocessing and feature engineering, trains and evaluates multiple machine learning models, and generates predictions. 

This pipeline is for the module CS611.

## Table of Contents
- [About](#-about)
- [Usage](#-how-to-build)

## ğŸš€ About
There are two main pipelines in this project: data_pipeline and ml_pipeline. Apache Spark and MLflow are used for the data and ml pipelines. Docker is used to containerize the environment for each pipeline. 

The file structure is shown below:
```
â”‚
â”œâ”€â”€ data/                          # Store the raw CSV files here
â”œâ”€â”€ datamart/                      # Jupyter notebooks for exploration and experimentation
â”‚   â”œâ”€â”€ bronze/                    # Ingested data for the bronze layer 
â”‚   â”œâ”€â”€ silver/                    # Cleaned data for the silver layer
â”‚   â””â”€â”€ gold/                      # Prepared data for the gold layer
â”‚
â”œâ”€â”€ etl/                           # Source code for the ETL pipeline
â”‚   â”œâ”€â”€ conf.yaml                  # Configuration file for the ETL pipeline
â”‚   â”œâ”€â”€ bronze_layer.py           # Scripts for raw data ingestion 
â”‚   â”œâ”€â”€ silver_layer.py           # Scripts for data cleaning
â”‚   â””â”€â”€ gold_layer.py             # Scripts for final data processing and dataset creation
â”‚
â”œâ”€â”€ ml/                            # Source code for the ML pipeline
â”‚   â”œâ”€â”€ optuna_config/            # Hyperparameter search configuration files
â”‚   â”‚   â”œâ”€â”€ logistic.yaml         # Optuna config for logistic regression
â”‚   â”‚   â””â”€â”€ xgboost.yaml          # Optuna config for XGBoost
â”‚   â”œâ”€â”€ data_loader.py            # Functions to load and prepare data for ML
â”‚   â”œâ”€â”€ preprocessor.py           # Data preprocessing utilities (scaling, encoding, etc.)
â”‚   â””â”€â”€ model_manager.py          # Model training, evaluation, and persistence logic
â”‚
â”œâ”€â”€ run_data_pipeline.py          # main script to run the data pipeline
â”œâ”€â”€ run_ml_pipeline.py            # main script to run the ml pipeline
â”œâ”€â”€ Dockerfile                    # Instructions to build the Docker image
â”œâ”€â”€ docker-compose.yml            # Define and run multi-container Docker applications
â”œâ”€â”€ .gitignore                    # Specify intentionally untracked files to ignore
â”œâ”€â”€ requirements.txt              # List of Python dependencies
â””â”€â”€ README.md                     # Project documentation and setup instructions


```

## ğŸ“ Usage


```
# Clone the repository, and install docker desktop

# In the project root, 
docker compose up

```

There are two containers: data_pipe_project, mlflow-server.

![Docker](assets/docker.png)

data_pipe_project is used as the main container to run the data and ml pipelines, mlflow-server provides the mlflow ui

## Running the ML Pipeline

```
in the project root, 
python run_ml_pipeline.py

```
The script will:
1. Load feature and label data
2. Perform time-based train-test-oot splits
3. Preprocess features for both logistic regression and XGBoost
4. Apply SMOTE or undersampling
5. Tune hyperparameters using Optuna
6. Evaluate models and log everything to MLflow

## MLflow Tracking
All experiment runs, models, parameters, and metrics are automatically logged to:
- mlruns/ folder (mounted volume)

Open MLflow UI at: http://localhost:5000

