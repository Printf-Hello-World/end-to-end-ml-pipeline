{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8a61b5",
   "metadata": {},
   "source": [
    "# ML Pipeline: Fraud Detection Training\n",
    "This notebook loads gold parquet data, performs time-based train/val/test/OOT splits, preprocesses data for Logistic Regression and XGBoost, handles class imbalance, trains models with Optuna-tuned hyperparameters, and evaluates them with MLflow logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b240074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import mlflow\n",
    "from glob import glob\n",
    "from src.data_loader import load_gold_parquet\n",
    "from src.data_splitter import time_based_split\n",
    "from src.features import preprocess_features\n",
    "from src.imbalance_handler import handle_imbalance\n",
    "from src.model_trainer import train_logistic_regression_tuned, train_xgboost_tuned\n",
    "from src.model_evaluator import evaluate_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086e480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MONTHS = pd.date_range(\"2017-01-01\", \"2019-10-01\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "CUTOFFS = {\n",
    "    \"oot1\": \"2018-11-01\",\n",
    "    \"oot2\": \"2019-03-01\",\n",
    "    \"oot3\": \"2019-07-01\"\n",
    "}\n",
    "FEATURE_DIR = \"/app/datamart/gold/feature_store\"\n",
    "LABEL_PATH = \"/app/datamart/gold/label_store/gold_labels.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbfb263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found parquet files: ['/app/datamart/gold/feature_store/gold_features2015_01_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_02_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_03_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_04_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_05_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_06_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_07_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_08_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_09_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_10_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_11_01.parquet', '/app/datamart/gold/feature_store/gold_features2015_12_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_01_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_02_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_03_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_04_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_05_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_06_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_07_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_08_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_09_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_10_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_11_01.parquet', '/app/datamart/gold/feature_store/gold_features2016_12_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_01_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_02_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_03_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_04_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_05_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_06_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_07_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_08_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_09_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_10_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_11_01.parquet', '/app/datamart/gold/feature_store/gold_features2017_12_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_01_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_02_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_03_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_04_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_05_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_06_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_07_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_08_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_09_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_10_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_11_01.parquet', '/app/datamart/gold/feature_store/gold_features2018_12_01.parquet', '/app/datamart/gold/feature_store/gold_features2019_01_01.parquet', '/app/datamart/gold/feature_store/gold_features2019_02_01.parquet', '/app/datamart/gold/feature_store/gold_features2019_03_01.parquet', '/app/datamart/gold/feature_store/gold_features2019_04_01.parquet', '/app/datamart/gold/feature_store/gold_features2019_05_01.parquet', '/app/datamart/gold/feature_store/gold_features2019_06_01.parquet', '/app/datamart/gold/feature_store/gold_features2019_07_01.parquet', '/app/datamart/gold/feature_store/gold_features2019_08_01.parquet', '/app/datamart/gold/feature_store/gold_features2019_09_01.parquet', '/app/datamart/gold/feature_store/gold_features2019_10_01.parquet']\n",
      "         id        date client_id card_id     amount            use_chip  \\\n",
      "0  15471192  2015-01-01       316    2038  69.550003    Chip Transaction   \n",
      "1  15471193  2015-01-01      1585     339  34.820000   Swipe Transaction   \n",
      "2  15471194  2015-01-01       848    3915  64.400002    Chip Transaction   \n",
      "3  15471195  2015-01-01      1797     300  47.930000    Chip Transaction   \n",
      "4  15471196  2015-01-01      1557    2471  25.750000  Online Transaction   \n",
      "\n",
      "  merchant_id merchant_city merchant_state      zip  ...  cvv has_chip  \\\n",
      "0       79360      giddings             tx  78942.0  ...  448      yes   \n",
      "1       69972  jacksonville             fl  32222.0  ...   63       no   \n",
      "2       13051       harwood             md  20776.0  ...  120      yes   \n",
      "3       54343   san leandro             ca  94577.0  ...  987      yes   \n",
      "4        9932        online         online      0.0  ...  978       no   \n",
      "\n",
      "  num_cards_issued  credit_limit  acct_open_date  year_pin_last_changed  \\\n",
      "0                2         10500      2002-03-01                   2010   \n",
      "1                1         14708      2014-01-01                   2014   \n",
      "2                1         19113      2009-07-01                   2014   \n",
      "3                1         16279      2012-01-01                   2012   \n",
      "4                1         12370      2004-04-01                   2008   \n",
      "\n",
      "   card_on_dark_web                        mcc_description acct_opened_months  \\\n",
      "0                no           grocery stores, supermarkets              156.3   \n",
      "1                no                  fast food restaurants               12.2   \n",
      "2                no  drinking places (alcoholic beverages)               67.0   \n",
      "3                no                taxicabs and limousines               36.5   \n",
      "4                no                      department stores              130.9   \n",
      "\n",
      "   yrs_since_pin_changed  \n",
      "0                      5  \n",
      "1                      1  \n",
      "2                      1  \n",
      "3                      3  \n",
      "4                      7  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "(6734248, 40)\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/app/datamart/gold/feature_store\"\n",
    "parquet_files = glob(os.path.join(folder_path, \"*.parquet\"))\n",
    "\n",
    "print(\"Found parquet files:\", parquet_files)  # Debugging\n",
    "\n",
    "df = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad24299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found parquet files: ['/app/datamart/gold/label_store/gold_labels.parquet/part-00000-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00001-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00002-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00003-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00004-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00005-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00006-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00007-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00008-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00009-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00010-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00011-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00012-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet', '/app/datamart/gold/label_store/gold_labels.parquet/part-00013-1ba3aad8-01ec-4321-8732-2547e7bebdd9-c000.snappy.parquet']\n",
      "  transaction_id is_fraud\n",
      "0       10477015       no\n",
      "1       20842753       no\n",
      "2       20703200       no\n",
      "3       18374734       no\n",
      "4       11603719       no\n",
      "(8914963, 2)\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/app/datamart/gold/label_store/gold_labels.parquet\"\n",
    "parquet_files = glob(os.path.join(folder_path, \"*.parquet\"))\n",
    "\n",
    "print(\"Found parquet files:\", parquet_files)  # Debugging\n",
    "\n",
    "df = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b37aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Size: (1029521, 40)\n",
      "[Val]   Size: (343174, 40)\n",
      "[Test]  Size: (343174, 40)\n",
      "[OOT1]  Size: (306433, 41)\n",
      "[OOT2]  Size: (312160, 41)\n",
      "[OOT3]  Size: (314760, 41)\n",
      "Date range in dataset:\n",
      "2017-01-01 00:00:00 → 2019-10-31 00:00:00\n",
      "Train columns: ['transaction_id', 'date', 'client_id', 'card_id', 'amount', 'use_chip', 'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc', 'errors', 'year_month', 'current_age', 'retirement_age', 'birth_year', 'birth_month', 'gender', 'address', 'latitude', 'longitude', 'per_capita_income', 'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards', 'card_brand', 'card_type', 'card_number', 'expires', 'cvv', 'has_chip', 'num_cards_issued', 'credit_limit', 'acct_open_date', 'year_pin_last_changed', 'card_on_dark_web', 'mcc_description', 'acct_opened_months', 'yrs_since_pin_changed']\n",
      "OOT1 columns: ['transaction_id', 'date', 'client_id', 'card_id', 'amount', 'use_chip', 'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc', 'errors', 'year_month', 'current_age', 'retirement_age', 'birth_year', 'birth_month', 'gender', 'address', 'latitude', 'longitude', 'per_capita_income', 'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards', 'card_brand', 'card_type', 'card_number', 'expires', 'cvv', 'has_chip', 'num_cards_issued', 'credit_limit', 'acct_open_date', 'year_pin_last_changed', 'card_on_dark_web', 'mcc_description', 'acct_opened_months', 'yrs_since_pin_changed']\n"
     ]
    }
   ],
   "source": [
    "# Load and split data\n",
    "df = load_gold_parquet(FEATURE_DIR, LABEL_PATH, MONTHS)\n",
    "splits = time_based_split(df, date_col=\"date\", target_col=\"is_fraud\", cutoffs=CUTOFFS)\n",
    "\n",
    "for key in splits:\n",
    "    splits[key] = (\n",
    "        splits[key][0],\n",
    "        splits[key][1].astype(str).str.lower().map({\"no\": 0, \"yes\": 1}).astype(int)\n",
    "    )\n",
    "\n",
    "X_train, y_train = splits[\"train\"]\n",
    "X_val, y_val = splits[\"val\"]\n",
    "X_test, y_test = splits[\"test\"]\n",
    "X_oot1, y_oot1 = splits[\"oot1\"]\n",
    "X_oot2, y_oot2 = splits[\"oot2\"]\n",
    "X_oot3, y_oot3 = splits[\"oot3\"]\n",
    "\n",
    "print(\"Date range in dataset:\")\n",
    "print(df[\"date\"].min(), \"→\", df[\"date\"].max())\n",
    "\n",
    "print(\"Train columns:\", X_train.columns.tolist())\n",
    "print(\"OOT1 columns:\", X_oot1.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc82aaa-6771-4cb2-96af-06914ab1c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution: [1028653     868]\n",
      "Validation label distribution: [342885    289]\n",
      "Test label distribution: [342885    289]\n",
      "OOT1 label distribution: [305836    597]\n",
      "OOT2 label distribution: [311578    582]\n",
      "OOT3 label distribution: [314224    536]\n"
     ]
    }
   ],
   "source": [
    "# Check the label distribution\n",
    "print(\"Train label distribution:\", np.bincount(y_train))\n",
    "print(\"Validation label distribution:\", np.bincount(y_val))\n",
    "print(\"Test label distribution:\", np.bincount(y_test))\n",
    "print(\"OOT1 label distribution:\", np.bincount(y_oot1))\n",
    "print(\"OOT2 label distribution:\", np.bincount(y_oot2))\n",
    "print(\"OOT3 label distribution:\", np.bincount(y_oot3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba78022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression pipeline\n",
    "print(\"[Step] Preprocessing features for Logistic Regression...\")\n",
    "X_train_lr, lr_pipeline = preprocess_features(X_train, model_type=\"logistic\", fit_pipeline=True)\n",
    "X_val_lr, _ = preprocess_features(X_val, model_type=\"logistic\", fit_pipeline=False, pipeline=lr_pipeline)\n",
    "X_test_lr, _ = preprocess_features(X_test, model_type=\"logistic\", fit_pipeline=False, pipeline=lr_pipeline)\n",
    "X_oot1_lr, _ = preprocess_features(X_oot1, model_type=\"logistic\", fit_pipeline=False, pipeline=lr_pipeline)\n",
    "print(\"[Done] Feature preprocessing completed.\")\n",
    "\n",
    "print(\"[Step] Handling imbalance with SMOTE...\")\n",
    "X_train_lr, y_train_lr = handle_imbalance(X_train_lr, y_train, strategy=\"smote\")\n",
    "print(\"[Done] SMOTE resampling completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment and start training\n",
    "print(\"[Step] Starting MLflow experiment...\")\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "mlflow.set_experiment(\"fraud_detection\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"LogisticRegression\"):\n",
    "    print(\"[Step] Training logistic regression with Optuna tuning...\")\n",
    "    logreg_model = train_logistic_regression_tuned(X_train_lr, y_train_lr, X_val_lr, y_val, pipeline=lr_pipeline, X_raw_train=X_train)\n",
    "    print(\"[Done] Logistic regression training completed.\")\n",
    "\n",
    "    print(\"[Step] Evaluating on test set...\")\n",
    "    evaluate_model(logreg_model, X_test_lr, y_test, model_name=\"LogReg\", dataset_label=\"Test\")\n",
    "    print(\"[Done] Test set evaluation completed.\")\n",
    "\n",
    "    # print(\"[Step] Evaluating on OOT1 set...\")\n",
    "    # evaluate_model(logreg_model, X_oot1_lr, y_oot1, model_name=\"LogReg\", dataset_label=\"OOT1\")\n",
    "    # print(\"[Done] OOT1 set evaluation completed.\")\n",
    "\n",
    "print(\"[ALL COMPLETE] Logistic regression pipeline executed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d645b011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step] Preprocessing features for XGBoost...\n",
      "[Done] Feature preprocessing completed.\n",
      "[Step] Handling imbalance for XGBoost...\n",
      "[Imbalance Handler] Strategy: undersample\n",
      "[Resampled] Samples: 1029521 → 1736 (Class 1: 868, Class 0: 868)\n",
      "[Done] Imbalance handling completed.\n"
     ]
    }
   ],
   "source": [
    "# XGBoost pipeline\n",
    "print(\"[Step] Preprocessing features for XGBoost...\")\n",
    "X_train_xgb, _, xgb_input_example = preprocess_features(X_train, model_type=\"xgboost\", return_sample=True)\n",
    "X_val_xgb, _ = preprocess_features(X_val, model_type=\"xgboost\")\n",
    "X_test_xgb, _ = preprocess_features(X_test, model_type=\"xgboost\")\n",
    "X_oot1_xgb, _ = preprocess_features(X_oot1, model_type=\"xgboost\")\n",
    "print(\"[Done] Feature preprocessing completed.\")\n",
    "\n",
    "print(\"[Step] Handling imbalance for XGBoost...\")\n",
    "X_train_xgb, y_train_xgb = handle_imbalance(X_train_xgb, y_train, strategy=\"undersample\")\n",
    "print(\"[Done] Imbalance handling completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c05e1a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step] Starting MLflow run for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 02:22:29,318] A new study created in memory with name: no-name-2d0b001e-ba5b-4f0d-b2a7-f677f2636c21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step] Training XGBoost with Optuna tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 02:22:34,057] Trial 0 finished with value: 0.9982807739810621 and parameters: {'n_estimators': 397, 'max_depth': 9, 'learning_rate': 0.1791196565096831, 'subsample': 0.8279480853078705, 'colsample_bytree': 0.6152226492958887}. Best is trial 0 with value: 0.9982807739810621.\n",
      "[I 2025-05-30 02:22:37,236] Trial 1 finished with value: 0.9982807739810621 and parameters: {'n_estimators': 271, 'max_depth': 8, 'learning_rate': 0.039616248610473574, 'subsample': 0.6662601004961927, 'colsample_bytree': 0.7407303950174817}. Best is trial 0 with value: 0.9982807739810621.\n",
      "[I 2025-05-30 02:22:40,808] Trial 2 finished with value: 0.9988571428571429 and parameters: {'n_estimators': 489, 'max_depth': 9, 'learning_rate': 0.19339196112090265, 'subsample': 0.8541924560299821, 'colsample_bytree': 0.6401276674551465}. Best is trial 2 with value: 0.9988571428571429.\n",
      "c:\\Users\\leeze\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [02:22:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation Performance - XGBoost]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9992    0.9999    0.9995    342885\n",
      "           1     0.0000    0.0000    0.0000       289\n",
      "\n",
      "    accuracy                         0.9990    343174\n",
      "   macro avg     0.4996    0.4999    0.4998    343174\n",
      "weighted avg     0.9983    0.9990    0.9987    343174\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leeze\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leeze\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [02:22:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fb952212d34b8695cdb3729d1fad05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] xgboost model -> artifacts\\xgboost_model.pkl\n",
      "[Done] XGBoost training completed.\n",
      "[Step] Evaluating XGBoost on test set...\n",
      "\n",
      "[Evaluating] XGBoost on Test set...\n",
      "Confusion Matrix:\n",
      " [[342851     34]\n",
      " [   289      0]]\n",
      "F1-score: 0.0000 | ROC AUC: 0.9980 | PR AUC: 0.1729\n",
      "[Done] Test set evaluation completed.\n",
      "[Step] Ending MLflow run for XGBoost...\n",
      "[Done] MLflow run ended.\n",
      "[ALL COMPLETE] XGBoost pipeline executed successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"[Step] Starting MLflow run for XGBoost...\")\n",
    "mlflow.set_experiment(\"fraud_detection\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGBoost\"):\n",
    "    print(\"[Step] Training XGBoost with Optuna tuning...\")\n",
    "    xgb_model = train_xgboost_tuned(\n",
    "        X_train_xgb, y_train_xgb, X_val_xgb, y_val, input_example=xgb_input_example\n",
    "    )\n",
    "    print(\"[Done] XGBoost training completed.\")\n",
    "\n",
    "    print(\"[Step] Evaluating XGBoost on test set...\")\n",
    "    evaluate_model(xgb_model, X_test_xgb, y_test, model_name=\"XGBoost\", dataset_label=\"Test\")\n",
    "    print(\"[Done] Test set evaluation completed.\")\n",
    "\n",
    "    # print(\"[Step] Evaluating XGBoost on OOT1 set...\")\n",
    "    # evaluate_model(xgb_model, X_oot1_xgb, y_oot1, model_name=\"XGBoost\", dataset_label=\"OOT1\")\n",
    "    # print(\"[Done] OOT1 set evaluation completed.\")\n",
    "\n",
    "    print(\"[Step] Ending MLflow run for XGBoost...\")\n",
    "    mlflow.end_run()\n",
    "    print(\"[Done] MLflow run ended.\")\n",
    "\n",
    "print(\"[ALL COMPLETE] XGBoost pipeline executed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9fed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution: [1028653     868]\n",
      "Val class distribution: [342885    289]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Class distribution check\n",
    "print(\"Train class distribution:\", np.bincount(y_train))\n",
    "print(\"Validation class distribution:\", np.bincount(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f2e2998",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Class prediction distribution\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted class distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mbincount(y_pred))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# AUC-PR\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Class prediction distribution\n",
    "y_pred = best_model.predict(X_val)\n",
    "print(\"Predicted class distribution:\", np.bincount(y_pred))\n",
    "\n",
    "# AUC-PR\n",
    "from sklearn.metrics import average_precision_score\n",
    "print(\"AUC-PR:\", average_precision_score(y_val, best_model.predict_proba(X_val)[:, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
